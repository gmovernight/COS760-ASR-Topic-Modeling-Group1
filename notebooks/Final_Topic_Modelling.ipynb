{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BERT AND LDA TOPIC MODELLING\n"
      ],
      "metadata": {
        "id": "ME6-QAtTwY8W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SBYhnM4vaWm"
      },
      "outputs": [],
      "source": [
        "#BERTopic Analysis with Dynamic Parameter Adjustment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install bertopic umap-learn hdbscan sentence-transformers wordcloud plotly gensim\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import plotly.express as px\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class BERTopicAnalyzer:\n",
        "    \"\"\"Complete BERTopic analyzer with dynamic parameter adjustment\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Extended stopword lists\n",
        "        self.english_stopwords = set(stopwords.words('english'))\n",
        "        self.setswana_stopwords = {'hore', 'le', 'ka', 'ya', 'ba', 'e', 'sa','te', 'di', 'ten', 'done', 'ga', 'bona', 'everyone', 'bo', 'went'}\n",
        "        self.asr_artifacts = {'hartha', 'ore', 'tele', 'um', 'uh', 'ah'}\n",
        "        self.custom_fillers = {'going', 'come', 'one', 'make', 'take', 'time', 'said'}\n",
        "        self.all_stopwords = (self.english_stopwords | self.setswana_stopwords |\n",
        "                             self.asr_artifacts | self.custom_fillers)\n",
        "\n",
        "    def load_transcripts(self, file_path):\n",
        "        \"\"\"Load and clean transcript data with robust parsing\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        if \"---\" in content or \"TRANSCRIPT\" in content.upper():\n",
        "            sections = re.split(r'\\n(?=---|\\w+\\s*TRANSCRIPT)', content, flags=re.IGNORECASE)\n",
        "            docs = [' '.join([line.strip() for line in section.split('\\n')\n",
        "                   if not line.strip().startswith('---') and 'TRANSCRIPT' not in line.upper()]).strip()\n",
        "                   for section in sections if section.strip()]\n",
        "        else:\n",
        "            docs = [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "\n",
        "        # Filter short documents and ensure minimum length\n",
        "        return [doc for doc in docs if len(doc) > 20]\n",
        "\n",
        "    def initialize_bertopic(self, n_docs):\n",
        "        \"\"\"Dynamically configure parameters based on dataset size\"\"\"\n",
        "        # 1. Embedding model (multilingual for better handling)\n",
        "        embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "        # 2. UMAP - adjust neighbors based on dataset size\n",
        "        umap_model = UMAP(\n",
        "            n_neighbors=min(15, max(5, n_docs//3)),  # Dynamic neighbor count\n",
        "            n_components=5,\n",
        "            min_dist=0.0,\n",
        "            metric='cosine',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 3. HDBSCAN - dynamic cluster size\n",
        "        hdbscan_model = HDBSCAN(\n",
        "            min_cluster_size=max(5, min(25, n_docs//5)),  # 5-25 range\n",
        "            metric='euclidean',\n",
        "            cluster_selection_method='eom',\n",
        "            prediction_data=True\n",
        "        )\n",
        "\n",
        "        # 4. Vectorizer - auto-adjust thresholds\n",
        "        vectorizer_model = CountVectorizer(\n",
        "            stop_words=list(self.all_stopwords),\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=max(1, min(3, n_docs//10)),  # 1-3 docs minimum\n",
        "            max_df=min(0.95, 0.7 + (n_docs/100))  # 0.7-0.95 range\n",
        "        )\n",
        "\n",
        "        return BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            language=\"multilingual\",\n",
        "            calculate_probabilities=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def calculate_coherence(self, model, docs):\n",
        "        \"\"\"Calculate coherence scores with robust error handling\"\"\"\n",
        "        try:\n",
        "            processed_texts = [[word for word in doc.lower().split()\n",
        "                              if word not in self.all_stopwords and len(word) > 2]\n",
        "                             for doc in docs]\n",
        "\n",
        "            dictionary = corpora.Dictionary(processed_texts)\n",
        "            topic_words = [[word for word, _ in model.get_topic(i)]\n",
        "                          for i in range(len(model.get_topic_info())-1)]\n",
        "\n",
        "            coherence_scores = {}\n",
        "            for metric in ['c_v', 'u_mass', 'c_npmi']:\n",
        "                try:\n",
        "                    cm = CoherenceModel(\n",
        "                        topics=topic_words,\n",
        "                        texts=processed_texts,\n",
        "                        dictionary=dictionary,\n",
        "                        coherence=metric\n",
        "                    )\n",
        "                    coherence_scores[metric] = cm.get_coherence()\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not calculate {metric}: {str(e)}\")\n",
        "                    coherence_scores[metric] = np.nan\n",
        "            return coherence_scores\n",
        "        except Exception as e:\n",
        "            print(f\"Coherence calculation failed: {str(e)}\")\n",
        "            return {'c_v': np.nan, 'u_mass': np.nan, 'c_npmi': np.nan}\n",
        "\n",
        "    def visualize_results(self, model, docs):\n",
        "        \"\"\"Generate all visualizations with error handling\"\"\"\n",
        "        try:\n",
        "            # Word Clouds\n",
        "            n_topics = len(model.get_topic_info())-1\n",
        "            plt.figure(figsize=(16, max(6, n_topics*2)))\n",
        "            for i in range(n_topics):\n",
        "                plt.subplot((n_topics//5)+1, 5, i+1)\n",
        "                topic_words = dict(model.get_topic(i))\n",
        "                wordcloud = WordCloud(width=300, height=200,\n",
        "                                    background_color='white',\n",
        "                                    colormap='viridis').generate_from_frequencies(topic_words)\n",
        "                plt.imshow(wordcloud)\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(f\"Topic {i}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Topic Distribution\n",
        "            topic_info = model.get_topic_info()\n",
        "            topic_info = topic_info[topic_info.Topic != -1]\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.barplot(x='Topic', y='Count', data=topic_info, palette='viridis')\n",
        "            plt.title('Document Distribution Across Topics')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Hierarchical Topics\n",
        "            hierarchical_topics = model.hierarchical_topics(docs)\n",
        "            fig = model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
        "            fig.update_layout(width=800, height=600)\n",
        "            fig.show()\n",
        "\n",
        "            # Topic Similarity\n",
        "            fig = model.visualize_heatmap()\n",
        "            fig.update_layout(width=800, height=800)\n",
        "            fig.show()\n",
        "\n",
        "            return topic_info\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Complete analysis workflow with robust error handling\"\"\"\n",
        "        try:\n",
        "            print(\"Upload your transcript file:\")\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise ValueError(\"No file uploaded\")\n",
        "\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            docs = self.load_transcripts(filename)\n",
        "            n_docs = len(docs)\n",
        "\n",
        "            print(f\"\\nLoaded {n_docs} documents\")\n",
        "            if n_docs < 5:\n",
        "                raise ValueError(f\"Only {n_docs} documents found. Need at least 5.\")\n",
        "\n",
        "            print(\"Sample document:\", docs[0][:100] + \"...\")\n",
        "\n",
        "            # Initialize with dynamic parameters\n",
        "            model = self.initialize_bertopic(n_docs)\n",
        "            topics, probs = model.fit_transform(docs)\n",
        "\n",
        "            # Calculate coherence\n",
        "            coherence_scores = self.calculate_coherence(model, docs)\n",
        "            print(\"\\nCOHERENCE SCORES:\")\n",
        "            print(f\"• C_V: {coherence_scores.get('c_v', 'NA'):.4f} (Higher is better, >0.4 good)\")\n",
        "            print(f\"• UMass: {coherence_scores.get('u_mass', 'NA'):.4f} (Closer to 0 is better)\")\n",
        "            print(f\"• NPMI: {coherence_scores.get('c_npmi', 'NA'):.4f} (Positive is good)\")\n",
        "\n",
        "            # Generate visualizations\n",
        "            topic_info = self.visualize_results(model, docs)\n",
        "\n",
        "            # Show topic details\n",
        "            if topic_info is not None:\n",
        "                print(\"\\nTOPIC DETAILS:\")\n",
        "                for idx, row in topic_info.iterrows():\n",
        "                    if row.Topic >= 0:\n",
        "                        words = [word for word, _ in model.get_topic(row.Topic)][:10]\n",
        "                        print(f\"\\nTopic {row.Topic} ({row.Count} docs): {', '.join(words)}\")\n",
        "\n",
        "            return model, topic_info, coherence_scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nERROR: {str(e)}\")\n",
        "            print(\"\\nTroubleshooting Tips:\")\n",
        "            if \"max_df\" in str(e):\n",
        "                print(\"- Try uploading more documents (10+ recommended)\")\n",
        "                print(\"- Or edit vectorizer parameters in initialize_bertopic()\")\n",
        "            elif \"documents\" in str(e):\n",
        "                print(\"- Check file format (one document per line or --- separated)\")\n",
        "                print(\"- Ensure documents have >20 characters of meaningful text\")\n",
        "            return None, None, None\n",
        "\n",
        "# Run the analysis\n",
        "print(\"Starting enhanced BERTopic analysis...\")\n",
        "analyzer = BERTopicAnalyzer()\n",
        "bertopic_model, bertopic_info, coherence_scores = analyzer.run_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LDA Analysis with Comprehensive Visualizations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install gensim pyLDAvis wordcloud\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class EnhancedLDAAnalyzer:\n",
        "    \"\"\"Optimized LDA analyzer with enhanced visualizations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.initialize_stopwords()\n",
        "        self.coherence_metrics = ['c_v', 'u_mass', 'c_npmi']\n",
        "\n",
        "    def initialize_stopwords(self):\n",
        "        \"\"\"Extended stopword lists for multilingual content\"\"\"\n",
        "        self.english_stopwords = set(stopwords.words('english'))\n",
        "        self.setswana_stopwords = {'hore', 'le', 'ka', 'ya', 'ba', 'e', 'sa'}\n",
        "        self.asr_artifacts = {'hartha', 'ore', 'tele', 'um', 'uh', 'ah'}\n",
        "        self.custom_fillers = {'going', 'come', 'one', 'make', 'take', 'time', 'said'}\n",
        "        self.all_stopwords = (self.english_stopwords | self.setswana_stopwords |\n",
        "                            self.asr_artifacts | self.custom_fillers)\n",
        "\n",
        "    def load_transcripts(self, file_path):\n",
        "        \"\"\"Load and segment transcript files\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        if \"---\" in content or \"TRANSCRIPT\" in content.upper():\n",
        "            sections = re.split(r'\\n(?=---|\\w+\\s*TRANSCRIPT)', content, flags=re.IGNORECASE)\n",
        "            return [' '.join([line.strip() for line in section.split('\\n')\n",
        "                    if not line.strip().startswith('---') and 'TRANSCRIPT' not in line.upper()]).strip()\n",
        "                    for section in sections if section.strip()]\n",
        "        else:\n",
        "            return [line.strip() for line in content.split('\\n') if line.strip() and len(line.strip()) > 20]\n",
        "\n",
        "    def preprocess_text(self, texts):\n",
        "        \"\"\"Enhanced text cleaning pipeline\"\"\"\n",
        "        processed_texts = []\n",
        "\n",
        "        for text in texts:\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', text)\n",
        "            text = re.sub(r'[^\\w\\s]|\\d+', ' ', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            try:\n",
        "                tokens = word_tokenize(text)\n",
        "            except:\n",
        "                tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "            tokens = [\n",
        "                token for token in tokens\n",
        "                if (token not in self.all_stopwords) and\n",
        "                   (len(token) > 2) and\n",
        "                   (token.isalpha())\n",
        "            ]\n",
        "\n",
        "            if len(tokens) > 3:\n",
        "                processed_texts.append(tokens)\n",
        "\n",
        "        return processed_texts\n",
        "\n",
        "    def run_lda_analysis(self, texts, topic_range=[3, 5, 7]):\n",
        "        \"\"\"Complete LDA workflow with optimized settings\"\"\"\n",
        "        dictionary = corpora.Dictionary(texts)\n",
        "        dictionary.filter_extremes(\n",
        "            no_below=3,\n",
        "            no_above=0.7,\n",
        "            keep_n=800\n",
        "        )\n",
        "\n",
        "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "        results = {}\n",
        "        for num_topics in topic_range:\n",
        "            lda_model = LdaModel(\n",
        "                corpus=corpus,\n",
        "                id2word=dictionary,\n",
        "                num_topics=num_topics,\n",
        "                random_state=42,\n",
        "                passes=20,\n",
        "                alpha='asymmetric',\n",
        "                eta=0.01,\n",
        "                iterations=400\n",
        "            )\n",
        "\n",
        "            coherence_scores = {}\n",
        "            for metric in self.coherence_metrics:\n",
        "                try:\n",
        "                    cm = CoherenceModel(\n",
        "                        model=lda_model,\n",
        "                        texts=texts,\n",
        "                        dictionary=dictionary,\n",
        "                        coherence=metric\n",
        "                    )\n",
        "                    coherence_scores[metric] = cm.get_coherence()\n",
        "                except Exception as e:\n",
        "                    print(f\"{metric.upper()} failed: {str(e)[:50]}\")\n",
        "                    coherence_scores[metric] = np.nan\n",
        "\n",
        "            results[num_topics] = {\n",
        "                'model': lda_model,\n",
        "                'coherence': coherence_scores,\n",
        "                'dictionary': dictionary,\n",
        "                'corpus': corpus\n",
        "            }\n",
        "\n",
        "            print(f\"\\nTopics: {num_topics}\")\n",
        "            for metric, score in coherence_scores.items():\n",
        "                if not np.isnan(score):\n",
        "                    print(f\"  {metric.upper()}: {score:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_coherence_plot(self, results):\n",
        "        \"\"\"Visualize coherence scores across topic numbers\"\"\"\n",
        "        topics = list(results.keys())\n",
        "        c_v = [res['coherence']['c_v'] for res in results.values()]\n",
        "        u_mass = [res['coherence']['u_mass'] for res in results.values()]\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(topics, c_v, 'o-', color='#1f77b4', label='C_V Coherence')\n",
        "        plt.plot(topics, u_mass, 's--', color='#ff7f0e', label='U_Mass Coherence')\n",
        "        plt.title('Topic Model Coherence Scores', fontsize=14)\n",
        "        plt.xlabel('Number of Topics', fontsize=12)\n",
        "        plt.ylabel('Coherence Score', fontsize=12)\n",
        "        plt.xticks(topics)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nCOHERENCE PLOT EXPLANATION:\")\n",
        "        print(\"• Shows how topic quality changes with different numbers of topics\")\n",
        "        print(\"• C_V (blue): Higher values (0.4+) indicate better topics\")\n",
        "        print(\"• U_Mass (orange): Values closer to 0 are better\")\n",
        "        print(\"• Optimal topic count is often at the 'elbow' point\")\n",
        "\n",
        "    def create_word_clouds(self, model, num_topics):\n",
        "        \"\"\"Generate word clouds for each topic\"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        plt.suptitle('Topic Word Clouds', fontsize=16)\n",
        "        for i in range(num_topics):\n",
        "            plt.subplot(2, (num_topics+1)//2, i+1)\n",
        "            topic_words = dict(model.show_topic(i, 30))\n",
        "            wordcloud = WordCloud(width=300, height=200,\n",
        "                                background_color='white',\n",
        "                                colormap='viridis').generate_from_frequencies(topic_words)\n",
        "            plt.imshow(wordcloud)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Topic {i}\", fontsize=10)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nWORD CLOUD EXPLANATION:\")\n",
        "        print(\"• Each cloud represents one topic\")\n",
        "        print(\"• Word size indicates importance in the topic\")\n",
        "        print(\"• Color intensity shows relative frequency\")\n",
        "        print(\"• Helps quickly identify dominant themes\")\n",
        "\n",
        "    def create_topic_distribution(self, corpus, model):\n",
        "        \"\"\"Show distribution of topics across documents\"\"\"\n",
        "        doc_topics = [model.get_document_topics(doc) for doc in corpus]\n",
        "        topic_counts = np.zeros(model.num_topics)\n",
        "\n",
        "        for doc in doc_topics:\n",
        "            if doc:\n",
        "                dominant_topic = max(doc, key=lambda x: x[1])[0]\n",
        "                topic_counts[dominant_topic] += 1\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        ax = sns.barplot(x=np.arange(model.num_topics), y=topic_counts, palette='viridis')\n",
        "        plt.title('Document Distribution Across Topics', fontsize=14)\n",
        "        plt.xlabel('Topic Number', fontsize=12)\n",
        "        plt.ylabel('Number of Documents', fontsize=12)\n",
        "\n",
        "        # Add value labels\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(f\"{int(p.get_height())}\",\n",
        "                       (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                       ha='center', va='center',\n",
        "                       xytext=(0, 5),\n",
        "                       textcoords='offset points')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nTOPIC DISTRIBUTION EXPLANATION:\")\n",
        "        print(\"• Shows how documents are distributed across topics\")\n",
        "        print(\"• Ideally should be relatively balanced\")\n",
        "        print(\"• Very large/small bars may indicate:\")\n",
        "        print(\"  - Topics that are too broad/narrow\")\n",
        "        print(\"  - Need to adjust topic numbers\")\n",
        "\n",
        "    def visualize_results(self, results):\n",
        "        \"\"\"Generate all visualizations with explanations\"\"\"\n",
        "        best_num = max([(num, res['coherence']['c_v'])\n",
        "                      for num, res in results.items()\n",
        "                      if not np.isnan(res['coherence']['c_v'])])[0]\n",
        "        best_model = results[best_num]['model']\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"VISUALIZING RESULTS FOR BEST MODEL ({best_num} TOPICS)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. Coherence Plot\n",
        "        print(\"\\nGenerating coherence plot...\")\n",
        "        self.create_coherence_plot(results)\n",
        "\n",
        "        # 2. Word Clouds\n",
        "        print(\"\\nGenerating word clouds...\")\n",
        "        self.create_word_clouds(best_model, best_num)\n",
        "\n",
        "        # 3. Topic Distribution\n",
        "        print(\"\\nGenerating topic distribution...\")\n",
        "        self.create_topic_distribution(results[best_num]['corpus'], best_model)\n",
        "\n",
        "        # 4. Interactive Visualization\n",
        "        print(\"\\nGenerating interactive visualization...\")\n",
        "        vis = gensimvis.prepare(\n",
        "            best_model,\n",
        "            results[best_num]['corpus'],\n",
        "            results[best_num]['dictionary']\n",
        "        )\n",
        "        pyLDAvis.display(vis)\n",
        "\n",
        "        print(\"\\nINTERACTIVE VISUALIZATION GUIDE:\")\n",
        "        print(\"• Left panel: Shows most relevant terms for selected topic\")\n",
        "        print(\"• Right panel (Intertopic Distance Map):\")\n",
        "        print(\"  - Circle size = topic prevalence\")\n",
        "        print(\"  - Distance between circles = topic similarity\")\n",
        "        print(\"  - λ slider adjusts term relevance (1=common, 0=distinctive)\")\n",
        "        print(\"• Hover over elements for detailed information\")\n",
        "\n",
        "def run_enhanced_analysis():\n",
        "    analyzer = EnhancedLDAAnalyzer()\n",
        "\n",
        "    print(\"Please upload your transcript file (TXT format):\")\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "\n",
        "    print(\"\\nLoading and preprocessing transcripts...\")\n",
        "    raw_texts = analyzer.load_transcripts(filename)\n",
        "    processed_texts = analyzer.preprocess_text(raw_texts)\n",
        "\n",
        "    print(f\"\\nInitial documents: {len(raw_texts)}\")\n",
        "    print(f\"Valid documents after preprocessing: {len(processed_texts)}\")\n",
        "\n",
        "    if len(processed_texts) < 5:\n",
        "        print(\"Error: Need at least 5 valid documents for analysis\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nRunning enhanced LDA analysis...\")\n",
        "    results = analyzer.run_lda_analysis(\n",
        "        processed_texts,\n",
        "        topic_range=[3, 5, 7]\n",
        "    )\n",
        "\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    analyzer.visualize_results(results)\n",
        "\n",
        "# Execute the analysis\n",
        "run_enhanced_analysis()"
      ],
      "metadata": {
        "id": "luAIUjcOvq8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}